{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165906b286f4dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.170315Z",
     "start_time": "2025-09-12T20:27:02.449553Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils as vutils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3903d6582d57f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.184136Z",
     "start_time": "2025-09-12T20:27:08.176494Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #[B, 3, 128, 128]\n",
    "            spectral_norm(nn.Conv2d(img_channels, base_channels, 4, stride=2, padding=1)), # [B, 64, 64, 64]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels, base_channels * 2, 4, stride=2, padding=1)), # [B, 128, 32, 32]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 2, base_channels * 4, 4, stride=2, padding=1)), # [B, 256, 16, 16]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 4, base_channels * 8, 4, stride=2, padding=1)), # [B, 512, 8, 8]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(base_channels * 8, 1, 4, stride=1, padding=0)), # [B, 1, 5, 5]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1) # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24311bf-0b59-4c23-aa05-6634fc311503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        c_ = max(in_ch // 8, 1)\n",
    "        self.query = nn.Conv2d(in_ch, c_, 1)\n",
    "        self.key   = nn.Conv2d(in_ch, c_, 1)\n",
    "        self.value = nn.Conv2d(in_ch, in_ch, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.scale = c_ ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)   # [B, HW, C/8]\n",
    "        k = self.key(x).view(B, -1, H * W)                      # [B, C/8, HW]\n",
    "        attn = torch.softmax(self.scale * (q @ k), dim=-1)      # [B, HW, HW]\n",
    "        v = self.value(x).view(B, C, H * W)                     # [B, C, HW]\n",
    "        out = (v @ attn.permute(0, 2, 1)).view(B, C, H, W)      # [B, C, H, W]\n",
    "        return self.gamma * out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384013d2-c088-4495-9946-19ec59a1d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_norm=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        ]\n",
    "        if use_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_ch))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
    "        if use_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_ch))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dda701b55ba34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.212639Z",
     "start_time": "2025-09-12T20:27:08.202885Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, img_channels=3, base_channels=64, resolution=128, attn_res=(32, 64)):\n",
    "        super().__init__()\n",
    "        assert resolution in (64, 128, 256)\n",
    "        num_ups = int(math.log2(resolution) - 2)\n",
    "\n",
    "        c0 = base_channels * 16\n",
    "        # [B, z_dim]\n",
    "        self.fc = nn.Linear(z_dim, 16 * c0)  # [B, 16c0]\n",
    "\n",
    "        chs = [c0]\n",
    "        for _ in range(num_ups):\n",
    "            chs.append(max(chs[-1] // 2, base_channels))\n",
    "\n",
    "        blocks = []\n",
    "        cur_res = 4\n",
    "        for i in range(num_ups):\n",
    "            seq = [GenBlock(chs[i], chs[i + 1], use_norm=(i < num_ups - 2))]\n",
    "            cur_res *= 2\n",
    "            if cur_res in set(attn_res):\n",
    "                seq.append(SelfAttention(chs[i + 1]))\n",
    "            blocks.append(nn.Sequential(*seq))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.Conv2d(chs[-1], img_channels, 3, padding=1),  # [B, 3, resolution, resolution]\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, z_dim]\n",
    "        x = self.fc(z).view(z.size(0), -1, 4, 4)  # [B, c0, 4, 4]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.to_rgb(x)  # [B, 3, H, W]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c673bcfb3e869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.224951Z",
     "start_time": "2025-09-12T20:27:08.218672Z"
    }
   },
   "outputs": [],
   "source": [
    "def d_hinge_loss(real_logits, fake_logits):\n",
    "    return torch.relu(1. - real_logits).mean() + torch.relu(1. + fake_logits).mean()\n",
    "\n",
    "def g_hinge_loss(fake_logits):\n",
    "    return (-fake_logits).mean()\n",
    "\n",
    "def r1_regularization(real_imgs, real_logits):\n",
    "    grad_real = torch.autograd.grad(\n",
    "        outputs=real_logits.sum(), inputs=real_imgs,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    r1 = grad_real.pow(2).reshape(grad_real.size(0), -1).sum(1).mean()\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e090ec5dd28e9ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.286125Z",
     "start_time": "2025-09-12T20:27:08.231777Z"
    }
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 128\n",
    "Z_DIM = 256\n",
    "G_CH = 128\n",
    "D_CH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LR_G = 1e-4\n",
    "LR_D = 4e-4\n",
    "BETAS = (0.0, 0.99)\n",
    "R1_GAMMA = 5\n",
    "R1_EVERY = 32\n",
    "ACCUM_STEPS = 4\n",
    "WORKERS = 4\n",
    "SEED = 42\n",
    "DATASET_DIR = \"stanford_cars\"\n",
    "SAMPLES_DIR = \"samples\"\n",
    "WEIGHTS_DIR = \"weights\"\n",
    "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1997533d09384f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.299434Z",
     "start_time": "2025-09-12T20:27:08.293407Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_loader(resolution=128):\n",
    "    T = transforms.Compose([\n",
    "        transforms.Resize(resolution),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    train = ImageFolder(root=DATASET_DIR, transform=T)\n",
    "    return DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, drop_last=False, persistent_workers=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5e54ad3717e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.318587Z",
     "start_time": "2025-09-12T20:27:08.311271Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def show_and_save_samples(G, z_fixed, outdir, epoch, nrow=None):\n",
    "    G.eval()\n",
    "    imgs = G(z_fixed)\n",
    "    path = os.path.join(outdir, f\"epoch_{epoch:03d}.png\")\n",
    "    vutils.save_image(imgs, path, nrow=nrow or int(math.sqrt(imgs.size(0))),\n",
    "                      normalize=True, value_range=(-1, 1))\n",
    "    grid = vutils.make_grid(imgs, nrow=nrow or int(math.sqrt(imgs.size(0))),\n",
    "                            normalize=True, value_range=(-1,1))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.axis('off'); plt.imshow(grid.permute(1,2,0).cpu())\n",
    "    plt.show()\n",
    "    G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481dfc338e4af22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:08.814604Z",
     "start_time": "2025-09-12T20:27:08.325531Z"
    }
   },
   "outputs": [],
   "source": [
    "G = Generator(z_dim=Z_DIM, img_channels=3, base_channels=G_CH, resolution=RESOLUTION).to(device)\n",
    "D = Discriminator(img_channels=3, base_channels=D_CH).to(device)\n",
    "\n",
    "optG = torch.optim.AdamW(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "optD = torch.optim.AdamW(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "\n",
    "scalerG = torch.amp.GradScaler(device)\n",
    "scalerD = torch.amp.GradScaler(device)\n",
    "\n",
    "loader = make_loader(RESOLUTION)\n",
    "z_fixed = torch.randn(36, Z_DIM, device=device)\n",
    "\n",
    "print(sum(p.numel() for p in G.parameters())/1e6, \"M params in G\")\n",
    "print(sum(p.numel() for p in D.parameters())/1e6, \"M params in D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff0fac2cdb8094",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-12T20:27:08.825853Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "resume_path = \"\" #os.path.abspath(f\"{WEIGHTS_DIR}/epoch_069.pt\")\n",
    "start_epoch = 1\n",
    "step = 0\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    ckpt = torch.load(resume_path, map_location=device)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    D.load_state_dict(ckpt[\"D\"])\n",
    "    optG.load_state_dict(ckpt[\"optG\"])\n",
    "    optD.load_state_dict(ckpt[\"optD\"])\n",
    "    scalerG.load_state_dict(ckpt[\"scalerG\"])\n",
    "    scalerD.load_state_dict(ckpt[\"scalerD\"])\n",
    "    start_epoch = ckpt[\"epoch\"]\n",
    "    print(f\"Resumed from {resume_path}, epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for real, _ in pbar:\n",
    "        real = real.to(device, non_blocking=True)\n",
    "\n",
    "        optD.zero_grad(set_to_none=True)\n",
    "        z = torch.randn(real.size(0), Z_DIM, device=device)\n",
    "\n",
    "        with torch.amp.autocast(device):\n",
    "            fake = G(z).detach()\n",
    "            real_logits = D(real)\n",
    "            fake_logits = D(fake)\n",
    "            d_loss = d_hinge_loss(real_logits, fake_logits)\n",
    "\n",
    "        scalerD.scale(d_loss / ACCUM_STEPS).backward()\n",
    "\n",
    "        if R1_EVERY > 0 and (step % R1_EVERY == 0):\n",
    "            real.requires_grad_(True)\n",
    "            real_logits_r1 = D(real)\n",
    "            r1 = r1_regularization(real, real_logits_r1)\n",
    "            r1_loss = (R1_GAMMA * 0.5) * r1\n",
    "            scalerD.scale(r1_loss / ACCUM_STEPS).backward()\n",
    "            real.requires_grad_(False)\n",
    "        else:\n",
    "            r1_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scalerD.step(optD)\n",
    "            scalerD.update()\n",
    "\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        z = torch.randn(real.size(0), Z_DIM, device=device)\n",
    "        with torch.amp.autocast(device):\n",
    "            fake = G(z)\n",
    "            fake_logits = D(fake)\n",
    "            g_loss = g_hinge_loss(fake_logits)\n",
    "\n",
    "        scalerG.scale(g_loss / ACCUM_STEPS).backward()\n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scalerG.step(optG); scalerG.update()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(d_loss=f\"{d_loss.item():.3f}\", g_loss=f\"{g_loss.item():.3f}\", r1=f\"{r1_loss.item():.3f}\")\n",
    "\n",
    "    show_and_save_samples(G, z_fixed, outdir=SAMPLES_DIR, epoch=epoch)\n",
    "    torch.save(\n",
    "        {\"G\": G.state_dict(), \"D\": D.state_dict(),\n",
    "         \"optG\": optG.state_dict(), \"optD\": optD.state_dict(),\n",
    "         \"scalerG\": scalerG.state_dict(), \"scalerD\": scalerG.state_dict(),\n",
    "         \"epoch\": epoch},\n",
    "        os.path.join(WEIGHTS_DIR, f\"epoch_{epoch:03d}.pt\")\n",
    "    )\n",
    "    print(f\"[Epoch {epoch}] d_loss={d_loss.item():.3f} g_loss={g_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf78923-f62a-4f6e-b9ef-048b83ccd471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
